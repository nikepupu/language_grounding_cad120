{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import pickle\n",
    "from transformers import *\n",
    "from sklearn.externals import joblib\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tensorboardX import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAD120Dataset(Dataset):\n",
    "    def __init__(self, feature_file,  caption_file , raw_caption_file=None, transform=None):\n",
    "\n",
    "        \n",
    "        self.caption = joblib.load(caption_file)\n",
    "        self.caption = [ cap.permute(1,0,2) for cap in self.caption ]\n",
    "        #self.raw_caption = joblib.load(raw_caption_file)\n",
    "        self.features = joblib.load(feature_file)\n",
    "        #self.features = [feature.cpu().numpy() for feature in self.features ]\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.caption)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        features = self.features[idx]\n",
    "        captions = self.caption[idx]\n",
    "        #raw_captions = self.raw_caption[idx]\n",
    "        \n",
    "       \n",
    "        sample = {\"features\": features, \"captions\":captions, \n",
    "                 # \"raw_captions\": raw_captions\n",
    "                 }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cad120 =  CAD120Dataset('features_subject1.bin','caption_subject1.bin', 'caption_words_subject1.bin' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = cad120[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 1, 8192])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size_image, input_size_word_embedding, \n",
    "                hidden_size, output_size,num_layers=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size_image = input_size_image\n",
    "        self.input_size_word_embedding = input_size_word_embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_image = nn.LSTM(input_size_image, hidden_size, num_layers,  bidirectional=True)\n",
    "        self.lstm_caption = nn.LSTM(input_size_word_embedding, hidden_size, num_layers,  bidirectional=True)\n",
    "      \n",
    "    \n",
    "    def forward(self, image_features, caption_features):\n",
    "        # Set initial states\n",
    "        \n",
    "        img_batch_len = len(image_features)\n",
    "        caption_batch_len = len(caption_features)\n",
    "        \n",
    " \n",
    "       \n",
    "        h0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        \n",
    "        h1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        c1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        OUT0 = []\n",
    "        OUT1 = []\n",
    "        hidden_embedding0 = []\n",
    "        hidden_embedding1 = []\n",
    "        for i in range(img_batch_len):\n",
    "            #print(image_features[i].shape)\n",
    "            h0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            c0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            \n",
    "            hidden_embedding0 = []\n",
    "            for j in range(image_features[i].shape[0]):\n",
    "                out0, (h0,c0) = self.lstm_image(image_features[i][j].view(1,1,-1), (h0,c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "                hidden_embedding0.append(h0)\n",
    "            hidden_embedding0 = torch.stack(hidden_embedding0)\n",
    "            \n",
    "            OUT0.append(hidden_embedding0)\n",
    "                \n",
    "        \n",
    "        \n",
    "            \n",
    "        for i in range(caption_batch_len):\n",
    "            h1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            c1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            hidden_embedding1 = []\n",
    "            \n",
    "            for j in range(caption_features[i].shape[0]):\n",
    "                out1, (h1, c1) = self.lstm_caption(caption_features[i][j].view(1,1,-1), (h1, c1))\n",
    "                hidden_embedding1.append(h1)\n",
    "            \n",
    "            hidden_embedding1 = torch.stack(hidden_embedding1)\n",
    "            OUT1.append(hidden_embedding1)\n",
    "        \n",
    "\n",
    "        OUT0 = torch.stack(OUT0)\n",
    "        OUT1 = torch.stack(OUT1)\n",
    "        OUT0 = OUT0.view(OUT0.shape[0],OUT0.shape[1],-1)\n",
    "        OUT1 = OUT1.view(OUT1.shape[0],OUT1.shape[1],-1)\n",
    "        return OUT0,OUT1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(512*4*4, 768, 256, 10,1).to(device)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), 0.01,momentum=0.9,weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def my_collate(batch):\n",
    "    img = [item[\"features\"] for item in batch]\n",
    "    caption = [item[\"captions\"] for item in batch]\n",
    "    #raw_caption = [item[\"raw_captions\"] for item in batch]\n",
    "    #caption = torch.LongTensor(caption)\n",
    "    return [img, caption]\n",
    "\n",
    "train_loader = DataLoader( CAD120Dataset('features_subjectall.bin', \\\n",
    "                                         'caption_subjectall.bin', \\\n",
    "                                         'caption_words_subjectall.bin'), batch_size=16, shuffle = True,\n",
    "                          collate_fn = my_collate)\n",
    "test_set = CAD120Dataset('features_subject5.bin','caption_subject5.bin')\n",
    "test_loader = DataLoader( test_set, batch_size = int(len(test_set)), shuffle = False,\n",
    "                          collate_fn = my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set = CAD120Dataset('features_subject3.bin','caption_subject3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, b in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "        imgs = b[0]\n",
    "        captions = b[1]\n",
    "       \n",
    "        img_feature, caption_feature  =  [torch.tensor(img).to(device) for img in imgs], \\\n",
    "                        [torch.tensor(caption).to(device) for caption in captions]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h_image, h_caption = model(img_feature, caption_feature)\n",
    "\n",
    "        loss = torch.tensor(0.0).cuda()\n",
    "        accuracy = 0\n",
    "\n",
    "    \n",
    "        norm_image = torch.norm(h_image, dim=2).unsqueeze(2)\n",
    "        norm_caption = torch.norm(h_caption, dim=2).unsqueeze(2)\n",
    "\n",
    "        h_image = h_image/norm_image\n",
    "        h_caption = h_caption/norm_caption\n",
    "\n",
    "        matrix = torch.einsum('ijk, npk -> ijpn', h_image, h_caption)\n",
    "\n",
    "        matrix = torch.max(matrix, 2).values\n",
    "        matrix = torch.sum(matrix,1)\n",
    "        \n",
    "        #print(matrix)\n",
    "        diag = torch.diag(matrix)\n",
    "        \n",
    "        \n",
    "        tmp_diag = (diag*torch.ones(matrix.shape[0],matrix.shape[0]).cuda()).T \n",
    "      \n",
    "        tmp_diag = tmp_diag + torch.eye(matrix.shape[0]).cuda()\n",
    "        plus_one_matrix = matrix+1\n",
    "\n",
    "        \n",
    "        loss += torch.sum(torch.clamp(plus_one_matrix - tmp_diag, 0))\n",
    "        \n",
    "        \n",
    "        m = torch.max(matrix,1)\n",
    "#         for i in range(len(m.values)):\n",
    "#             index = m.indices[i]\n",
    "#             if index == i:\n",
    "#                 continue\n",
    "#             loss += 3*max(m.values[i] + 1 - diag[i], 0.0)\n",
    "        \n",
    "        accuracy1 = torch.sum( diag  >= m.values)*1.0 /  float(len(imgs))\n",
    "        accuracy += torch.sum( diag  >= m.values)*1.0\n",
    "        \n",
    "        \n",
    "        matrix = torch.einsum('ijk, npk -> ijpn', h_caption, h_image)\n",
    "        matrix = torch.max(matrix, 2).values\n",
    "        matrix = torch.sum(matrix,1)\n",
    "        #print(matrix)\n",
    "        #print(matrix)\n",
    "        diag = torch.diag(matrix)\n",
    "        tmp_diag = (diag*torch.ones(matrix.shape[0],matrix.shape[0]).cuda()).T \n",
    "        tmp_diag = tmp_diag + torch.eye(matrix.shape[0]).cuda()\n",
    "        plus_one_matrix = matrix+1\n",
    "        \n",
    "        loss += torch.sum(torch.clamp(plus_one_matrix - tmp_diag, 0))\n",
    "     \n",
    "        m = torch.max(matrix,1)\n",
    "#         for i in range(len(m.values)):\n",
    "#             index = m.indices[i]\n",
    "#             if index == i:\n",
    "#                 continue\n",
    "#             loss += max(m.values[i]+1-diag[i],0.0)\n",
    "\n",
    "        accuracy2 = torch.sum( diag  >= m.values)*1.0 / float(len(imgs))\n",
    "        accuracy += torch.sum( diag  >= m.values)*1.0\n",
    "\n",
    "        accuracy /= 2.0* float(len(imgs))\n",
    "        loss /= 2.0* float(len(imgs))\n",
    "        \n",
    "        loss = loss.cuda()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f} \\t Time: {:.4f} seconds Acc1: {:.4f} Acc2: {:.4f} Acc: {:.4f}'.format(\n",
    "                epoch, (batch_idx) * len(imgs), len(train_loader.dataset),\n",
    "                100. * (batch_idx) / len(train_loader), loss.item(), time.time()-start_time, accuracy1, accuracy2, accuracy))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader,epoch):\n",
    "    model.eval()\n",
    "    for batch_idx, b in enumerate(test_loader):\n",
    "        start_time = time.time()\n",
    "        imgs = b[0]\n",
    "        captions = b[1]\n",
    "       \n",
    "        img_feature, caption_feature  =  [torch.tensor(img).to(device) for img in imgs], \\\n",
    "                        [torch.tensor(caption).to(device) for caption in captions]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h_image, h_caption = model(img_feature, caption_feature)\n",
    "\n",
    "        loss = torch.tensor(0.0).cuda()\n",
    "        accuracy = 0\n",
    "    \n",
    "        norm_image = torch.norm(h_image, dim=2).unsqueeze(2)\n",
    "        norm_caption = torch.norm(h_caption, dim=2).unsqueeze(2)\n",
    "        \n",
    "        h_image = h_image/norm_image\n",
    "        h_caption = h_caption/norm_caption\n",
    "\n",
    "        \n",
    "        matrix = torch.einsum('ijk, npk -> ijpn', h_image, h_caption)\n",
    "        matrix = torch.max(matrix, 2).values\n",
    "        matrix = torch.sum(matrix,1)\n",
    "        diag = torch.diag(matrix)\n",
    "        tmp_diag = (diag*torch.ones(matrix.shape[0],matrix.shape[0]).cuda()).T \n",
    "        \n",
    "        #tmp_diag = tmp_diag + torch.eye(matrix.shape[0]).cuda()\n",
    "        plus_one_matrix = matrix+1\n",
    "\n",
    "\n",
    "        loss += torch.sum(torch.clamp(plus_one_matrix - tmp_diag, 0))\n",
    "        \n",
    "        \n",
    "        m = torch.max(matrix,1)\n",
    "#         for i in range(len(m.values)):\n",
    "#             index = m.indices[i]\n",
    "#             if index == i:\n",
    "#                 continue\n",
    "#             loss += 3*max(m.values[i] + 1 - diag[i], 0.0)\n",
    "        \n",
    "        accuracy1 = torch.sum( diag  > m.values)*1.0 /  float(len(imgs))\n",
    "        accuracy += torch.sum( diag  > m.values)*1.0\n",
    "        \n",
    "        \n",
    "        matrix = torch.einsum('ijk, npk -> ijpn', h_caption, h_image)\n",
    "        matrix = torch.max(matrix, 2).values\n",
    "        matrix = torch.sum(matrix,1)\n",
    "        diag = torch.diag(matrix)\n",
    "        tmp_diag = (diag*torch.ones(matrix.shape[0],matrix.shape[0]).cuda()).T \n",
    "        #tmp_diag = tmp_diag + torch.eye(matrix.shape[0]).cuda()\n",
    "        plus_one_matrix = matrix+1\n",
    "  \n",
    "        loss += torch.sum(torch.clamp(plus_one_matrix - tmp_diag, 0))\n",
    "     \n",
    "        m = torch.max(matrix,1)\n",
    "#         for i in range(len(m.values)):\n",
    "#             index = m.indices[i]\n",
    "#             if index == i:\n",
    "#                 continue\n",
    "#             loss += max(m.values[i]+1-diag[i],0.0)\n",
    "\n",
    "        accuracy2 = torch.sum( diag  > m.values)*1.0 / float(len(imgs))\n",
    "        accuracy += torch.sum( diag  > m.values)*1.0\n",
    "\n",
    "        accuracy /= 2.0* float(len(imgs))\n",
    "        loss /= 2.0* float(len(imgs))\n",
    "        print(\"different accuracy :\", accuracy1, accuracy2)\n",
    "\n",
    "        return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/455 (0%)]\tLoss: 34.2204 \t Time: 15.1515 seconds Acc1: 0.0625 Acc2: 0.0625 Acc: 0.0625\n",
      "Train Epoch: 0 [16/455 (3%)]\tLoss: 136.8487 \t Time: 15.6379 seconds Acc1: 0.0625 Acc2: 0.0000 Acc: 0.0312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2a4c17d98df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#writer.add_scalars('data/loss', {'loss': loss}, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#writer.add_scalars('data/train_accuracy', {'train_accuracy': accuracy}, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4689d31cfb80>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mine/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mine/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    train(model, 'cuda:0', train_loader, optimizer, epoch)\n",
    "    #writer.add_scalars('data/loss', {'loss': loss}, epoch)\n",
    "    #writer.add_scalars('data/train_accuracy', {'train_accuracy': accuracy}, epoch)\n",
    "    accuracy,loss = test(model, 'cuda:0', test_loader,epoch)\n",
    "    #print(\"test_accuracy: \", accuracy, \"test_loss\", loss)\n",
    "    torch.save(model.state_dict(), \"./checkPoint.pth\")\n",
    "    #writer.add_scalars('data/test_accuracy', {'test_accuracy': accuracy}, epoch)\n",
    "    #writer.add_scalars('data/test_loss', {'test_loss': loss}, epoch)\n",
    "    #if epoch == 500:\n",
    "        #optimizer = optim.SGD(model.parameters(), 0.01,momentum=0.9,weight_decay=1e-4)\n",
    "#writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "mine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
