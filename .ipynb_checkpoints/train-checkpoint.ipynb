{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import pickle\n",
    "from transformers import *\n",
    "from sklearn.externals import joblib\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tensorboardX import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAD120Dataset(Dataset):\n",
    "    def __init__(self, feature_file,  caption_file , raw_caption_file=None, transform=None):\n",
    "\n",
    "        \n",
    "        self.caption = joblib.load(caption_file)\n",
    "        self.caption = [ cap.permute(1,0,2) for cap in self.caption ]\n",
    "        #self.raw_caption = joblib.load(raw_caption_file)\n",
    "        self.features = joblib.load(feature_file)\n",
    "        #self.features = [feature.cpu().numpy() for feature in self.features ]\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.caption)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        features = self.features[idx]\n",
    "        captions = self.caption[idx]\n",
    "        #raw_captions = self.raw_caption[idx]\n",
    "        \n",
    "       \n",
    "        sample = {\"features\": features, \"captions\":captions, \n",
    "                 # \"raw_captions\": raw_captions\n",
    "                 }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cad120 =  CAD120Dataset('features_subject1.bin','caption_subject1.bin', 'caption_words_subject1.bin' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = cad120[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 1, 8192])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size_image, input_size_word_embedding, \n",
    "                hidden_size, output_size,num_layers=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size_image = input_size_image\n",
    "        self.input_size_word_embedding = input_size_word_embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_image = nn.LSTM(input_size_image, hidden_size, num_layers,  bidirectional=True)\n",
    "        self.lstm_caption = nn.LSTM(input_size_word_embedding, hidden_size, num_layers,  bidirectional=True)\n",
    "      \n",
    "    \n",
    "    def forward(self, image_features, caption_features):\n",
    "        # Set initial states\n",
    "        \n",
    "        img_batch_len = len(image_features)\n",
    "        caption_batch_len = len(caption_features)\n",
    "        \n",
    " \n",
    "       \n",
    "        h0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        \n",
    "        h1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        c1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        OUT0 = []\n",
    "        OUT1 = []\n",
    "        hidden_embedding0 = []\n",
    "        hidden_embedding1 = []\n",
    "        for i in range(img_batch_len):\n",
    "            #print(image_features[i].shape)\n",
    "            h0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            c0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            \n",
    "            hidden_embedding0 = []\n",
    "            for j in range(image_features[i].shape[0]):\n",
    "                out0, (h0,c0) = self.lstm_image(image_features[i][j].view(1,1,-1), (h0,c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "                hidden_embedding0.append(h0)\n",
    "            hidden_embedding0 = torch.stack(hidden_embedding0)\n",
    "            \n",
    "            OUT0.append(hidden_embedding0)\n",
    "                \n",
    "        \n",
    "        \n",
    "            \n",
    "        for i in range(caption_batch_len):\n",
    "            h1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            c1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            hidden_embedding1 = []\n",
    "            \n",
    "            for j in range(caption_features[i].shape[0]):\n",
    "                out1, (h1, c1) = self.lstm_caption(caption_features[i][j].view(1,1,-1), (h1, c1))\n",
    "                hidden_embedding1.append(h1)\n",
    "            \n",
    "            hidden_embedding1 = torch.stack(hidden_embedding1)\n",
    "            OUT1.append(hidden_embedding1)\n",
    "        \n",
    "\n",
    "        OUT0 = torch.stack(OUT0)\n",
    "        OUT1 = torch.stack(OUT1)\n",
    "        OUT0 = OUT0.view(OUT0.shape[0],OUT0.shape[1],-1)\n",
    "        OUT1 = OUT1.view(OUT1.shape[0],OUT1.shape[1],-1)\n",
    "        return OUT0,OUT1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(512*4*4, 768, 256, 10,1).to(device)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), 0.01,momentum=0.9,weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def my_collate(batch):\n",
    "    img = [item[\"features\"] for item in batch]\n",
    "    caption = [item[\"captions\"] for item in batch]\n",
    "    #raw_caption = [item[\"raw_captions\"] for item in batch]\n",
    "    #caption = torch.LongTensor(caption)\n",
    "    return [img, caption]\n",
    "\n",
    "train_loader = DataLoader( CAD120Dataset('features_subjectall.bin', \\\n",
    "                                         'caption_subjectall.bin', \\\n",
    "                                         'caption_words_subjectall.bin'), batch_size=16, shuffle = True,\n",
    "                          collate_fn = my_collate)\n",
    "test_set = CAD120Dataset('features_subject5.bin','caption_subject5.bin')\n",
    "test_loader = DataLoader( test_set, batch_size = int(len(test_set)), shuffle = False,\n",
    "                          collate_fn = my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set = CAD120Dataset('features_subject3.bin','caption_subject3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, b in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "        imgs = b[0]\n",
    "        captions = b[1]\n",
    "       \n",
    "        img_feature, caption_feature  =  [torch.tensor(img).to(device) for img in imgs], \\\n",
    "                        [torch.tensor(caption).to(device) for caption in captions]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h_image, h_caption = model(img_feature, caption_feature)\n",
    "\n",
    "        loss = torch.tensor(0.0).cuda()\n",
    "        accuracy = 0\n",
    "\n",
    "    \n",
    "        norm_image = torch.norm(h_image, dim=2).unsqueeze(2)\n",
    "        norm_caption = torch.norm(h_caption, dim=2).unsqueeze(2)\n",
    "\n",
    "        h_image = h_image/norm_image\n",
    "        h_caption = h_caption/norm_caption\n",
    "\n",
    "        matrix = torch.einsum('ijk, npk -> ijpn', h_image, h_caption)\n",
    "\n",
    "        matrix = torch.max(matrix, 2).values\n",
    "        matrix = torch.sum(matrix,1)\n",
    "        \n",
    "        print(matrix)\n",
    "        diag = torch.diag(matrix)\n",
    "        \n",
    "        \n",
    "        tmp_diag = (diag*torch.ones(matrix.shape[0],matrix.shape[0]).cuda()).T \n",
    "      \n",
    "        #tmp_diag = tmp_diag + torch.eye(matrix.shape[0]).cuda()\n",
    "        plus_one_matrix = matrix+1\n",
    "\n",
    "        \n",
    "        loss += torch.sum(torch.clamp(plus_one_matrix - tmp_diag, 0))\n",
    "        \n",
    "        \n",
    "        m = torch.max(matrix,1)\n",
    "#         for i in range(len(m.values)):\n",
    "#             index = m.indices[i]\n",
    "#             if index == i:\n",
    "#                 continue\n",
    "#             loss += 3*max(m.values[i] + 1 - diag[i], 0.0)\n",
    "        \n",
    "        accuracy1 = torch.sum( diag  >= m.values)*1.0 /  float(len(imgs))\n",
    "        accuracy += torch.sum( diag  >= m.values)*1.0\n",
    "        \n",
    "        \n",
    "        matrix = torch.einsum('ijk, npk -> ijpn', h_caption, h_image)\n",
    "        matrix = torch.max(matrix, 2).values\n",
    "        matrix = torch.sum(matrix,1)\n",
    "        print(matrix)\n",
    "        #print(matrix)\n",
    "        diag = torch.diag(matrix)\n",
    "        tmp_diag = (diag*torch.ones(matrix.shape[0],matrix.shape[0]).cuda()).T \n",
    "        #tmp_diag = tmp_diag + torch.eye(matrix.shape[0]).cuda()\n",
    "        plus_one_matrix = matrix+1\n",
    "        \n",
    "        loss += torch.sum(torch.clamp(plus_one_matrix - tmp_diag, 0))\n",
    "     \n",
    "        m = torch.max(matrix,1)\n",
    "#         for i in range(len(m.values)):\n",
    "#             index = m.indices[i]\n",
    "#             if index == i:\n",
    "#                 continue\n",
    "#             loss += max(m.values[i]+1-diag[i],0.0)\n",
    "\n",
    "        accuracy2 = torch.sum( diag  >= m.values)*1.0 / float(len(imgs))\n",
    "        accuracy += torch.sum( diag  >= m.values)*1.0\n",
    "\n",
    "        accuracy /= 2.0* float(len(imgs))\n",
    "        loss /= 2.0* float(len(imgs))\n",
    "        \n",
    "        loss = loss.cuda()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f} \\t Time: {:.4f} seconds Acc1: {:.4f} Acc2: {:.4f} Acc: {:.4f}'.format(\n",
    "                epoch, (batch_idx) * len(imgs), len(train_loader.dataset),\n",
    "                100. * (batch_idx) / len(train_loader), loss.item(), time.time()-start_time, accuracy1, accuracy2, accuracy))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader,epoch):\n",
    "    model.eval()\n",
    "    for batch_idx, b in enumerate(test_loader):\n",
    "        start_time = time.time()\n",
    "        imgs = b[0]\n",
    "        captions = b[1]\n",
    "       \n",
    "        img_feature, caption_feature  =  [torch.tensor(img).to(device) for img in imgs], \\\n",
    "                        [torch.tensor(caption).to(device) for caption in captions]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h_image, h_caption = model(img_feature, caption_feature)\n",
    "\n",
    "        loss = torch.tensor(0.0).cuda()\n",
    "        accuracy = 0\n",
    "    \n",
    "        norm_image = torch.norm(h_image, dim=2).unsqueeze(2)\n",
    "        norm_caption = torch.norm(h_caption, dim=2).unsqueeze(2)\n",
    "        \n",
    "        h_image = h_image/norm_image\n",
    "        h_caption = h_caption/norm_caption\n",
    "\n",
    "        \n",
    "        matrix = torch.einsum('ijk, npk -> ijpn', h_image, h_caption)\n",
    "        matrix = torch.max(matrix, 2).values\n",
    "        matrix = torch.sum(matrix,1)\n",
    "        diag = torch.diag(matrix)\n",
    "        tmp_diag = (diag*torch.ones(matrix.shape[0],matrix.shape[0]).cuda()).T \n",
    "        \n",
    "        #tmp_diag = tmp_diag + torch.eye(matrix.shape[0]).cuda()\n",
    "        plus_one_matrix = matrix+1\n",
    "\n",
    "\n",
    "        loss += torch.sum(torch.clamp(plus_one_matrix - tmp_diag, 0))\n",
    "        \n",
    "        \n",
    "        m = torch.max(matrix,1)\n",
    "#         for i in range(len(m.values)):\n",
    "#             index = m.indices[i]\n",
    "#             if index == i:\n",
    "#                 continue\n",
    "#             loss += 3*max(m.values[i] + 1 - diag[i], 0.0)\n",
    "        \n",
    "        accuracy1 = torch.sum( diag  > m.values)*1.0 /  float(len(imgs))\n",
    "        accuracy += torch.sum( diag  > m.values)*1.0\n",
    "        \n",
    "        \n",
    "        matrix = torch.einsum('ijk, npk -> ijpn', h_caption, h_image)\n",
    "        matrix = torch.max(matrix, 2).values\n",
    "        matrix = torch.sum(matrix,1)\n",
    "        diag = torch.diag(matrix)\n",
    "        tmp_diag = (diag*torch.ones(matrix.shape[0],matrix.shape[0]).cuda()).T \n",
    "        #tmp_diag = tmp_diag + torch.eye(matrix.shape[0]).cuda()\n",
    "        plus_one_matrix = matrix+1\n",
    "  \n",
    "        loss += torch.sum(torch.clamp(plus_one_matrix - tmp_diag, 0))\n",
    "     \n",
    "        m = torch.max(matrix,1)\n",
    "#         for i in range(len(m.values)):\n",
    "#             index = m.indices[i]\n",
    "#             if index == i:\n",
    "#                 continue\n",
    "#             loss += max(m.values[i]+1-diag[i],0.0)\n",
    "\n",
    "        accuracy2 = torch.sum( diag  > m.values)*1.0 / float(len(imgs))\n",
    "        accuracy += torch.sum( diag  > m.values)*1.0\n",
    "\n",
    "        accuracy /= 2.0* float(len(imgs))\n",
    "        loss /= 2.0* float(len(imgs))\n",
    "        print(\"different accuracy :\", accuracy1, accuracy2)\n",
    "\n",
    "        return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/455 (0%)]\tLoss: 29.6581 \t Time: 15.1550 seconds Acc1: 0.0625 Acc2: 0.0625 Acc: 0.0625\n",
      "Train Epoch: 0 [16/455 (3%)]\tLoss: 111.9897 \t Time: 15.1874 seconds Acc1: 0.0625 Acc2: 0.2500 Acc: 0.1562\n",
      "Train Epoch: 0 [32/455 (7%)]\tLoss: 143.0460 \t Time: 15.1870 seconds Acc1: 0.0625 Acc2: 0.1875 Acc: 0.1250\n",
      "Train Epoch: 0 [48/455 (10%)]\tLoss: 87.5818 \t Time: 15.2005 seconds Acc1: 0.0625 Acc2: 0.4375 Acc: 0.2500\n",
      "Train Epoch: 0 [64/455 (14%)]\tLoss: 62.1266 \t Time: 15.1876 seconds Acc1: 0.1250 Acc2: 0.8125 Acc: 0.4688\n",
      "Train Epoch: 0 [80/455 (17%)]\tLoss: 48.5125 \t Time: 15.1972 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [96/455 (21%)]\tLoss: 23.9376 \t Time: 15.1949 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [112/455 (24%)]\tLoss: 25.0096 \t Time: 15.1818 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [128/455 (28%)]\tLoss: 27.3678 \t Time: 15.1983 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [144/455 (31%)]\tLoss: 24.2943 \t Time: 15.2023 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [160/455 (34%)]\tLoss: 21.4018 \t Time: 15.8209 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [176/455 (38%)]\tLoss: 18.3721 \t Time: 15.8191 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [192/455 (41%)]\tLoss: 18.7327 \t Time: 15.2755 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [208/455 (45%)]\tLoss: 18.5622 \t Time: 15.2405 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [224/455 (48%)]\tLoss: 19.7395 \t Time: 15.7838 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [240/455 (52%)]\tLoss: 18.0085 \t Time: 16.0163 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [256/455 (55%)]\tLoss: 17.1976 \t Time: 15.5605 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [272/455 (59%)]\tLoss: 16.5399 \t Time: 15.1959 seconds Acc1: 0.0000 Acc2: 1.0000 Acc: 0.5000\n",
      "Train Epoch: 0 [288/455 (62%)]\tLoss: 16.3515 \t Time: 15.2997 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [304/455 (66%)]\tLoss: 16.6132 \t Time: 15.7160 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 0 [320/455 (69%)]\tLoss: 14.6866 \t Time: 15.3064 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 0 [336/455 (72%)]\tLoss: 15.4221 \t Time: 15.2283 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 0 [352/455 (76%)]\tLoss: 14.7235 \t Time: 15.2140 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 0 [368/455 (79%)]\tLoss: 15.0465 \t Time: 15.2289 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 0 [384/455 (83%)]\tLoss: 16.2809 \t Time: 15.2135 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 0 [400/455 (86%)]\tLoss: 14.8769 \t Time: 15.2202 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 0 [416/455 (90%)]\tLoss: 13.2638 \t Time: 15.2393 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 0 [432/455 (93%)]\tLoss: 14.8655 \t Time: 15.2369 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 0 [196/455 (97%)]\tLoss: 6.2224 \t Time: 6.6825 seconds Acc1: 0.4286 Acc2: 1.0000 Acc: 0.7143\n",
      "different accuracy : tensor(0., device='cuda:0') tensor(0., device='cuda:0')\n",
      "Train Epoch: 1 [0/455 (0%)]\tLoss: 13.5593 \t Time: 15.2262 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 1 [16/455 (3%)]\tLoss: 13.0705 \t Time: 15.2457 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 1 [32/455 (7%)]\tLoss: 13.8969 \t Time: 15.2509 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 1 [48/455 (10%)]\tLoss: 12.1599 \t Time: 15.2438 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 1 [64/455 (14%)]\tLoss: 13.7632 \t Time: 15.2511 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 1 [80/455 (17%)]\tLoss: 14.2189 \t Time: 15.2472 seconds Acc1: 0.3750 Acc2: 1.0000 Acc: 0.6875\n",
      "Train Epoch: 1 [96/455 (21%)]\tLoss: 13.8541 \t Time: 15.2329 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 1 [112/455 (24%)]\tLoss: 11.6762 \t Time: 15.2283 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 1 [128/455 (28%)]\tLoss: 12.2583 \t Time: 15.2365 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 1 [144/455 (31%)]\tLoss: 14.2387 \t Time: 15.2271 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 1 [160/455 (34%)]\tLoss: 12.7477 \t Time: 15.2406 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 1 [176/455 (38%)]\tLoss: 12.3275 \t Time: 15.2504 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 1 [192/455 (41%)]\tLoss: 13.2123 \t Time: 15.2648 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 1 [208/455 (45%)]\tLoss: 12.6544 \t Time: 15.2543 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 1 [224/455 (48%)]\tLoss: 11.0189 \t Time: 15.2453 seconds Acc1: 0.6250 Acc2: 1.0000 Acc: 0.8125\n",
      "Train Epoch: 1 [240/455 (52%)]\tLoss: 11.1835 \t Time: 15.2363 seconds Acc1: 0.3750 Acc2: 1.0000 Acc: 0.6875\n",
      "Train Epoch: 1 [256/455 (55%)]\tLoss: 12.5929 \t Time: 15.2393 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 1 [272/455 (59%)]\tLoss: 11.3316 \t Time: 15.2281 seconds Acc1: 0.3750 Acc2: 1.0000 Acc: 0.6875\n",
      "Train Epoch: 1 [288/455 (62%)]\tLoss: 17.7614 \t Time: 15.2571 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 1 [304/455 (66%)]\tLoss: 13.8191 \t Time: 15.2596 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 1 [320/455 (69%)]\tLoss: 11.6227 \t Time: 15.2683 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 1 [336/455 (72%)]\tLoss: 12.8649 \t Time: 15.2471 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 1 [352/455 (76%)]\tLoss: 13.2513 \t Time: 15.2428 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 1 [368/455 (79%)]\tLoss: 13.2878 \t Time: 15.2489 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 1 [384/455 (83%)]\tLoss: 17.0437 \t Time: 15.2470 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 1 [400/455 (86%)]\tLoss: 13.3360 \t Time: 15.2466 seconds Acc1: 0.0625 Acc2: 1.0000 Acc: 0.5312\n",
      "Train Epoch: 1 [416/455 (90%)]\tLoss: 11.7862 \t Time: 15.2650 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 1 [432/455 (93%)]\tLoss: 12.1282 \t Time: 15.2695 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 1 [196/455 (97%)]\tLoss: 4.7911 \t Time: 6.6873 seconds Acc1: 0.5714 Acc2: 1.0000 Acc: 0.7857\n",
      "different accuracy : tensor(0., device='cuda:0') tensor(0., device='cuda:0')\n",
      "Train Epoch: 2 [0/455 (0%)]\tLoss: 14.7036 \t Time: 15.2283 seconds Acc1: 0.4375 Acc2: 1.0000 Acc: 0.7188\n",
      "Train Epoch: 2 [16/455 (3%)]\tLoss: 11.5692 \t Time: 15.2346 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 2 [32/455 (7%)]\tLoss: 10.8102 \t Time: 15.2232 seconds Acc1: 0.4375 Acc2: 1.0000 Acc: 0.7188\n",
      "Train Epoch: 2 [48/455 (10%)]\tLoss: 12.1743 \t Time: 15.2472 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 2 [64/455 (14%)]\tLoss: 11.9693 \t Time: 15.2528 seconds Acc1: 0.3750 Acc2: 1.0000 Acc: 0.6875\n",
      "Train Epoch: 2 [80/455 (17%)]\tLoss: 12.3036 \t Time: 15.2554 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 2 [96/455 (21%)]\tLoss: 12.1671 \t Time: 15.2163 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 2 [112/455 (24%)]\tLoss: 12.4815 \t Time: 15.2419 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 2 [128/455 (28%)]\tLoss: 11.6451 \t Time: 15.2457 seconds Acc1: 0.3750 Acc2: 1.0000 Acc: 0.6875\n",
      "Train Epoch: 2 [144/455 (31%)]\tLoss: 11.3385 \t Time: 15.2517 seconds Acc1: 0.5000 Acc2: 1.0000 Acc: 0.7500\n",
      "Train Epoch: 2 [160/455 (34%)]\tLoss: 12.5302 \t Time: 15.2252 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 2 [176/455 (38%)]\tLoss: 11.4127 \t Time: 15.2519 seconds Acc1: 0.1250 Acc2: 1.0000 Acc: 0.5625\n",
      "Train Epoch: 2 [192/455 (41%)]\tLoss: 10.3739 \t Time: 15.2433 seconds Acc1: 0.4375 Acc2: 1.0000 Acc: 0.7188\n",
      "Train Epoch: 2 [208/455 (45%)]\tLoss: 11.5470 \t Time: 15.2557 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 2 [224/455 (48%)]\tLoss: 11.3253 \t Time: 15.2511 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 2 [240/455 (52%)]\tLoss: 11.1142 \t Time: 15.2377 seconds Acc1: 0.3750 Acc2: 1.0000 Acc: 0.6875\n",
      "Train Epoch: 2 [256/455 (55%)]\tLoss: 11.5558 \t Time: 15.2405 seconds Acc1: 0.1875 Acc2: 1.0000 Acc: 0.5938\n",
      "Train Epoch: 2 [272/455 (59%)]\tLoss: 10.9872 \t Time: 15.2452 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [288/455 (62%)]\tLoss: 12.1493 \t Time: 15.2423 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 2 [304/455 (66%)]\tLoss: 12.6141 \t Time: 15.2578 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 2 [320/455 (69%)]\tLoss: 11.3953 \t Time: 15.2580 seconds Acc1: 0.3750 Acc2: 1.0000 Acc: 0.6875\n",
      "Train Epoch: 2 [336/455 (72%)]\tLoss: 11.3802 \t Time: 15.5824 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 2 [352/455 (76%)]\tLoss: 10.7518 \t Time: 16.1476 seconds Acc1: 0.4375 Acc2: 1.0000 Acc: 0.7188\n",
      "Train Epoch: 2 [368/455 (79%)]\tLoss: 11.7303 \t Time: 15.9944 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 2 [384/455 (83%)]\tLoss: 11.2881 \t Time: 15.9857 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 2 [400/455 (86%)]\tLoss: 11.2558 \t Time: 15.8656 seconds Acc1: 0.3125 Acc2: 1.0000 Acc: 0.6562\n",
      "Train Epoch: 2 [416/455 (90%)]\tLoss: 12.9030 \t Time: 15.9470 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n",
      "Train Epoch: 2 [432/455 (93%)]\tLoss: 11.8430 \t Time: 15.7298 seconds Acc1: 0.2500 Acc2: 1.0000 Acc: 0.6250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2a4c17d98df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#writer.add_scalars('data/loss', {'loss': loss}, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#writer.add_scalars('data/train_accuracy', {'train_accuracy': accuracy}, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-e49ee2c136f8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mh_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_caption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mine/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d0d489fdc159>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_features, caption_features)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mhidden_embedding0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mout0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# out: tensor of shape (batch_size, seq_length, hidden_size*2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mhidden_embedding0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mhidden_embedding0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_embedding0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mine/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mine/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mine/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mine/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    train(model, 'cuda:0', train_loader, optimizer, epoch)\n",
    "    #writer.add_scalars('data/loss', {'loss': loss}, epoch)\n",
    "    #writer.add_scalars('data/train_accuracy', {'train_accuracy': accuracy}, epoch)\n",
    "    accuracy,loss = test(model, 'cuda:0', test_loader,epoch)\n",
    "    #print(\"test_accuracy: \", accuracy, \"test_loss\", loss)\n",
    "    torch.save(model.state_dict(), \"./checkPoint.pth\")\n",
    "    #writer.add_scalars('data/test_accuracy', {'test_accuracy': accuracy}, epoch)\n",
    "    #writer.add_scalars('data/test_loss', {'test_loss': loss}, epoch)\n",
    "    #if epoch == 500:\n",
    "        #optimizer = optim.SGD(model.parameters(), 0.01,momentum=0.9,weight_decay=1e-4)\n",
    "#writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "mine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
